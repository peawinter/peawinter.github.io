---
published: false
title: STAT 520 Time Series Notes
layout: post
author: Wenyu
category:
- Notes
tags:
-  Notes
---

# Chapter 2: Fundamental Concepts

Stochastic Process
:    A sequence of r.v. indexed by time.

For example
:    $\{Y_t: t=1, 2, 3, ...\}$
$\{Y_t: t=0, \pm 1, \pm 2, \pm 3, ...\}$
$\{Y_t: t \in R\}$

- used to model time series as a data generating process
	- time series is a realization (sample) of S.P.

- The complete probability structure is determined by the dist. of all finite sets of $Y$'s, e.g. $\{t_1, ..., t_n\} \rightarrow \{Y_{t_1}, ... Y_{t_n}\}$

- Usually assume multivairate normal dist $\Rightarrow$ Gaussian Process, whose prob structure is completed specified by the first and second monents.

Mean function of S.P.
:    $\{Y_t, t = 1, 2, ...\}$ is $\mu_t = E[Y_t]$

The autocovariance function of S.P. $\{Y_t, t = 1, 2\}$
:    $\gamma_{r, s} = Cov(Y_t, Y_s)$

> Properties:
$\begin{align*}
\gamma_{t, t} & = Var(Y_t) \\
\gamma_{s, t} & = \gamma_{t, s} \\
|\gamma_{t, s}| & \le \sqrt{\gamma_{t, t} \gamma_{s, s}}
\end{align*}$

The autocorrelation function of $\{Y_t, t = 1, 2, ...\}$
:    $\rho = Corr(Y_t, Y_s) = {\gamma_{t, s} \over \sqrt{\gamma_{t, t} \gamma_{s, s}}}$

> Note: With constant $c_1, ..., c_m$ and $d_1, ..., d_n$, we have
$Cov(\sum_i^m{c_i Y_{t_i}}, \sum_j^n{d_j Y_{s_j}})
=\sum_i^m \sum_j^n c_i d_j cov(Y_{t_i}, Y_{s_j})$

> Properites:
$\begin{align*}
\rho_{t, t} & = 1 \\
\rho_{t, s} & = \rho_{s, t} \\
|\rho_{t, s}| & \le 1
\end{align*}$

## Random Walk

Suppose $e_i \sim N(0, \sigma_e^2)$

$$Y_1 = e_1, Y_t = Y_{t-1} + e_t$$

$e_t$: stepsize at time $t$
$Y_t$: position at tiem $t$

$\Rightarrow$ $\{y_t, t=1, 2, ...\}$ is a random walk.

> Note: $Y_t = Y_{t-1} + e_t = \sum_i e_i$

- Mean function
$$\mu_t = E[Y_t] = 0$$
- Autocovariance function
$$\gamma_{t, s} = Cov(Y_t, Y_s) = \min(t, s) \cdot \sigma_e^2$$
$\Rightarrow$ $\gamma_{t, t} = t \sigma_e^2$
- Autocorrelation function
$\Rightarrow$ $\rho_{t, s} = \frac{\min(t, s)}{\sqrt{ts}}=\sqrt{\frac{\min(t, s)}{\max(t, s)}}$

## Moving Average

Suppose $e_i \sim N(0, \sigma_e^2), i = 0, 1, 2, ...$
$$Y_t = \frac{e_t + e_{t-1}}{2}$$

- Mean:
$$\mu_t = E[Y_t] = 0$$
- Autovariance:
$$\gamma_{s, t} = Cov(Y_t, Y_s) =
\begin{cases}
{\sigma^2 \over 2}, & \textbf{if } |t-s| = 0 \\
{\sigma^2 \over 4}, & \textbf{if } |t-s|=1 \\
0, & \textbf{otherwise}
\end{cases}$$
- Autocorrelation:
$$\rho_{t, s} = \begin{cases}
1, & \textbf{if } |t-s| = 0 \\
0.5, & \textbf{if } |t-s|=1 \\
0, & \textbf{otherwise}
\end{cases}$$

## Stationarity
Strictly stationary
:    A S.P. $\{Y_t\}$ is strictly stationary if for any choice of time point, $\{t_1, ... ,t_n\}$ and $k \ge 0$, the joint dist of $\{Y_{t_1}, ..., Y_{t_n}\}$ is the same as the joint dist of $\{Y_{t_1 + k}, ..., Y_{t_n + k}\}$

$$[Y_{t_1}, ..., Y_{t_n}] = [Y_{t_1 + k}, ..., Y_{t_n + k}]$$

- Special case when $n=1$:

    Both mean and variance are the same for $Y_t$ and $Y_s$

- Special case when $n=2$:

    $$[Y_t, Y_s] = [Y_{t+k}, Y_{s+k}] = [Y_0, Y_{s-t}]$$

    $\Rightarrow$ $ \gamma_{t, s} = \gamma_{0, s - t}$

    In general, $\forall s, t > 0, \gamma_{t, s} = \gamma_{0, |t-s|}$

    $\Rightarrow \gamma_k = Cov(y_t, y_{t+k})$ and $\rho_k = \frac{\gamma_k}{\gamma_0}$

Thm. If $\{Y_t\}$ is strickly stationary, then
(1). $\mu_t$ is constant
(2). variance function ${Var(Y_t)}$ is also constant.
(3). $\gamma_{t, s} = \gamma_{0, |t-s|} = \gamma_{|t-s|}$
(4). $\rho_{t, s} = \rho_{0, |t-s|}=\frac{\gamma_{|t-s|}}{\gamma_0}$

Weakly Stationary
:    A S.P. $\{Y_t\}$ is weakly or second-order stationary if
1. The mean function is constant over time
2. The autocovariance function $\gamma_{t+k, k}=\gamma_{0, k}$, $\forall t$ and $k\ge 0$.

> In general, strictly stationary $\Rightarrow$ weakly statioanry
For Gaussian process, strictly stationary $\Leftrightarrow$ weakly stationary.
> Note: From now on "stationary" is "weakly stationary"

### Review

Time sequence $\{Y_t\}$

- Mean function: $\mu_t$
- Autocovariance function: $\gamma_{t, s}$
- Autocorrelation function: $\rho_{t,s}$
- Strickly Stationary
- (Weakly) Stationary
    - $\mu_t = \mu$
    - $\gamma_{t,s} = \gamma_{|t-s|}$ $\Rightarrow$ $\gamma_{t,s} = \gamma_{|t-s|}$

## White Noise Process

White noise $\{e_t\}$ with $e_t$ iid

> Strictly stationary

$\forall t_1, ..., t_n,  \& \ k$

$\begin{align*}
& P(e_{t_1}\le x_1, ..., e_{t_n} \le x_n) \\
= & \prod P(e_{t_i} \le x_i) \\
= & \prod P(e_{t_i +k} \le x_i) \\
= & P(e_{t_1 + k} \le x_1, ..., e_{t_n + k} \le x_n)
\end{align*}$

- $\mu_t = E[e_t] = const$
- $\gamma_k = \begin{cases} Var(e_t), & k = 0 \\ 0, & k \neq 0 \end{cases}$
- $\rho_k = {\gamma_k \over \gamma_0 }= \begin{cases}
1 & k=0 \\
0 & otherwise
\end{cases}$

> Its spectrum is analogous to that of white light $\Rightarrow$ "White Noise"

- Usually assume $\mu_t = 0, Var(e_t) = \sigma_e ^ 2$

## Moving average

$Y_t = \frac{e_t + e_{t - 1}}{2}$

Recall $\mu_t = 0$

$\gamma_{t, s} =
    \begin{cases}
       \frac{\sigma^2}{2}, & |t-s| = 0 \\
       \frac{\sigma^2}{4}, & |t-s| = 1\\
       0, & |t-s| > 1
    \end{cases}
$

> $\Rightarrow$ Stationary

## Random Cosine Wave

$Y_t = cos[2 \pi (\frac{t}{12} + \phi)]$, where $\phi \sim Unif[0, 1]$.

- Expectation function:
$\begin{align*}
\mu_t & = E[cos[2 \pi (\frac{t}{12} + \phi)]] \\
& = \int_0^1 cos[2 \pi (\frac{t}{12} + \phi)] d\phi \\
& = \frac{1}{2\pi} sin[2 \pi (\frac{t}{12} + \phi) ] \\
& = 0
\end{align*}$

- Covariance function:
$\begin{align*}
\gamma_{t, s} & = E[Y_t, Y_s] \\
& = \int_0^1 cos[2\pi(\frac{t}{12} + \phi)] cos[2\pi(\frac{s}{12} + \phi)] d\phi \\
& = \frac{1}{2} \int_0^1 cos[2\pi\frac{t-s}{12}] + cos[2\pi (\frac{t+s}{12} + 2\phi)] d\phi \\
& = \frac{1}{2} cos[2\pi \frac{t-s}{12}] \\
& = \gamma_{|t-s|}
\end{align*}$

> $\Rightarrow$ Stationary

Note: Difficult to check stationarity.

## Random Walk

$Y_t = \sum_{i = 1}^t e_i$ with

$\begin{cases}
\mu_t & = 0 \\
Var(Y_t) & = t \sigma_e^2 \\
\gamma_{t, s} & = \min\{t, s\} \cdot \sigma_e^2
\end{cases}
$

> Not stationary.

However, $DY_t =Y_t - Y_{t-1} = e_t \Rightarrow$ Difference Series is staionary

- Differencing & other transformations may be conducted on non-stationary series $\Rightarrow$ Stationary series

# Chapter 3: Trends

A time series is a sample of a stochastic process.

- $\mu_t$
- $\gamma_{t, s}$

Consider $Y_t = \mu_t + X_t$, where $\mu_t$ is the trend and $x$ is stationary. Assume $\mu_t = \mu$ (const mean).

$\begin{cases}
\mu_t^Y & = \mu_t + \mu_t^X \\
\gamma_{t, s} ^ Y & = \gamma_{t, s} ^ X \\
\rho_{t, s}^Y & = \rho_{t, s}^X
\end{cases}$

## Estimate of constant mean

Assume $Y_t = \mu + X_t$ and $\{X_t\}$ is stationary with $\mu_t^X = 0$.

Question
:    Given observed time series, $Y_1, ..., Y_n$, how to estimate $\mu$?

> Note: without further assumptions on $\{\gamma_k^x\}$, we'd better NOT to seek the optimal est of $\mu$

Answer
:    Least squares est $\min\{\sum(y_t - \mu)^2\} \Rightarrow \hat{\mu} = \bar{y}$
    $E[\hat{\mu}] = E[\bar{Y}] = \mu$ unbiased
    $\begin{align*}
Var(\hat{\mu}) & = Var(\frac{1}{n}\sum Y_t) = \frac{1}{n^2} Var(\sum Y_t) \\
& = \frac{1}{n^2} \{ n Var(Y_t) + 2\sum_{t=1}^n \sum_{s=1}^{t-1} Cov(Y_t, Y_s) \} \\
& = \frac{1}{n^2} \{ n\gamma_0 + 2 \sum_{t=1}^{n} \sum_{k=1}^{t-1} \gamma_k\} \\
& = \frac{1}{n^2} \{ n \gamma_0 + 2 \sum_{k=1}^{n-1}(n-k)\gamma_k\} \\
& = \frac{\gamma_0}{n} \{ 1 + 2 \sum_{k=1}^{n-1} (1 -\frac{k}{n} ) \rho_k \}
\end{align*}$

Example, $\{x_t\}$ is a white noise process $\Rightarrow$ $Var(\hat{\mu}) = \frac{\gamma_0}{n}$

> Note: If $\rho_k \ge 0, \forall k \ge 1$, then $Var(\hat{\mu}) \ge \frac{\gamma_0}{n}$

In many stationary process, we have $\sum_{k=0}^\infty |\rho_k| < \infty$ (a.c. function decays sufficiently with increasing lags)

When n is sufficiently large, we can approximate $Var(\hat{\mu}) \approx \frac{\gamma_0}{n} \sum_{k=-\infty}^{\infty} \rho_k$

example: $\rho_k = \phi^{|k|}$ with $|\phi| < 1$, then $Var(\hat{\mu}) \approx \frac{\gamma_0}{n}\frac{1+\phi}{1-\phi}$

>>> reviewed

## Polynomial trend

$\mu_t = \sum_{k=0} ^ {P} \beta_k t^k, P\ge1$

Model: $y_t = \mu_t + x_t$, $\{x_t\}$ is stationary with $\mu_t^x = 0$

Data (time series): $y_1, ..., y_n$ $n>>p$

- Least square est

$\min_{\beta_k} \{ \sum_{t=1}^n (y_t - \sum_{k = 0}^P \beta_k t^k)^2\}$
